# Partie 1 "Problem Solving"


## 1. Vacance et bagage
---

### Implementation
---
Cette impl√©mentation utilise la m√©thode de programmation dynamique pour r√©soudre ce probl√®me, qui est un exemple classique du probl√®me du "sac √† dos" (Knapsack Problem).


```python
def take_objects(obj_importance:list[int], obj_sizes:list[int],car_space:int) -> list[int]:
  if(len(obj_importance)!=len(obj_sizes)):
    raise ValueError("obj_importance and obj_sizes must have the same length")
  n = len(obj_importance)
  dp = [[0 for _ in range(car_space+1)] for _ in range(n+1)]
  # Remplir le tableau dp
  for i in range(1, n + 1):
      for w in range(car_space + 1):
          if obj_sizes[i-1] <= w:
              dp[i][w] = max(dp[i-1][w], dp[i-1][w - obj_sizes[i-1]] + obj_importance[i-1])
          else:
              dp[i][w] = dp[i-1][w]

  # Retrouver les objets pris
  w = car_space
  taken_objects = []
  for i in range(n, 0, -1):
      if dp[i][w] != dp[i-1][w]:
          taken_objects.append(i-1)
          w -= obj_sizes[i-1]

  return taken_objects[::-1]

```

cet algorithme est optimal pour r√©soudre ce probl√®me.

La complexit√© temporelle de l'algorithme est **ùëÇ(ùëõ √ó car_space)** et la complexit√© en espace est √©galement  **ùëÇ(ùëõ √ó car_space)**


### Test simple
---
Dans cette section, nous allons tester l'algorithme avec un cas simple pour v√©rifier son bon fonctionnement. Nous utilisons un exemple avec quatre objets ayant diff√©rentes tailles et importances, et une capacit√© de voiture limit√©e √† 10


```python
obj_importance = [10, 40, 30, 50]
obj_sizes = [5, 4, 6, 3]
car_space = 10
take_objects(obj_importance, obj_sizes, car_space)

```




    [1, 3]



### Test Case
---
Dans cette section, nous allons tester l'algorithme avec plusieurs cas d'essai stock√©s dans un fichier texte. Le fichier contient diff√©rentes configurations d'objets, de tailles, et de capacit√© de voiture, ainsi que les r√©sultats attendus pour chaque cas. Nous utilisons ces cas pour √©valuer l'exactitude de l'algorithme en comparant les r√©sultats obtenus avec ceux attendus. Le fichier de test est lu, chaque cas est trait√©, et la fonction take_objects est ex√©cut√©e pour chaque sc√©nario. Les r√©sultats obtenus sont ensuite compar√©s aux r√©sultats attendus pour d√©terminer si l'algorithme passe ou √©choue √† chaque test.

Ce processus garantit que l'algorithme fonctionne correctement pour une vari√©t√© de sc√©narios.


```python
def read_test_cases(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    test_cases = []
    for i in range(0, len(lines), 5):  # Assuming each test case has 5 lines (including blank lines)
        obj_importance = list(map(int, lines[i].strip().split()))
        obj_sizes = list(map(int, lines[i+1].strip().split()))
        car_space = int(lines[i+2].strip())

        # Process expected result (handle multiple solutions and empty lists)
        expected_result_line = lines[i+3].strip()
        if "Expected output is empty" in expected_result_line:
            expected_result = [[]]  # Expecting an empty list
        else:
            expected_result = [list(map(int, sol.strip().split())) for sol in expected_result_line.split('|')]

        test_cases.append((obj_importance, obj_sizes, car_space, expected_result))

    return test_cases
```


```python
def evaluate_function(function, test_cases):
    total_tests = len(test_cases)
    passed_tests = 0
    failed_tests = []

    for index, (obj_importance, obj_sizes, car_space, expected_results) in enumerate(test_cases):
        result = function(obj_importance, obj_sizes, car_space)

        # Check if the result matches any of the expected results
        if any(sorted(result) == sorted(expected) for expected in expected_results):
            passed_tests += 1
        else:
            failed_tests.append((index, result, expected_results))

    # Report results
    if passed_tests == total_tests:
        print(f"\033[92mCorrect Answer! Test passed {passed_tests}/{total_tests}\033[0m")
    else:
        print(f"\033[91mWrong Answer! Test passed {passed_tests}/{total_tests}\033[0m")
        
    if failed_tests:
        print("\nFailed Test Cases:")
        for index, result, expected_results in failed_tests:
            print(f"Test case {index + 1}:")
            print(f"  Got: {result}")
            print(f"  Expected: {expected_results}")
```

 ‚ö†Ô∏è Attention : 

Veuillez noter que pour ex√©cuter correctement la section des **Test Cases**, le fichier de test doit √™tre pr√©sent dans le chemin sp√©cifi√© : `TestCase/vac_bag.txt`. Si ce fichier n'est pas trouv√©, la cellule contenant le test ne fonctionnera pas correctement. Cependant, cela **n'affectera pas l'ex√©cution des autres cellules** du notebook, et l'algorithme continuera de fonctionner normalement pour les autres sections. Assurez-vous que le fichier est correctement plac√© pour √©viter toute erreur dans cette section sp√©cifique.


```python
# Load test cases
test_cases = read_test_cases("Testcase/vac_bag.txt")

# Evaluate the function
evaluate_function(take_objects, test_cases)
```

    [92mCorrect Answer! Test passed 28/28[0m


### Testez votre propre exemple
---
√Ä pr√©sent, c'est √† vous de jouer ! Entrez votre propre cas de test ci-dessous et voyez comment l'algorithme r√©agit avec votre exemple.

## 2. Fusionner plusieurs listes tri√©es
---


```python
# Definition for singly-linked list.
class ListNode:
    def __init__(self, val=0, next=None):
        self.val = val
        self.next = next
```


```python
import heapq
def mergeKLists(lists):

    heap = []
        
    # Add the head of each list to the heap (only non-empty lists)
    for i, l in enumerate(lists):
        if l:
            heapq.heappush(heap, (l.val, i, l))
        
    # Dummy node to help build the merged linked list
    dummy = ListNode()
    current = dummy
        
    # Extract the smallest element from the heap, and advance that list
    while heap:
        # Pop the smallest element (based on node's value)
        val, i, node = heapq.heappop(heap)
            
        # Add the smallest node to the result list
        current.next = node
        current = current.next
            
        # If there is a next node in the list, add it to the heap
        if node.next:
            heapq.heappush(heap, (node.next.val, i, node.next))
        
    # Return the merged linked list, starting from dummy's next node
    return dummy.next
```


```python
l1 = ListNode(1, ListNode(4, ListNode(5)))
l2 = ListNode(1, ListNode(3, ListNode(4)))
l3 = ListNode(2, ListNode(6))

merged_list = mergeKLists([l1, l2, l3])

# Function to print the merged linked list
def print_list(node):
    while node:
        print(node.val, end=" -> " if node.next else "")
        node = node.next
    print()

# Print the merged list
print_list(merged_list)
```

    1 -> 1 -> 2 -> 3 -> 4 -> 4 -> 5 -> 6


## 3. Permutation
---


```python
def permute(nums: list[int]) -> list[list[int]]:
  result = []

  def permute_rec(nums, current_index, result):
    if current_index == len(nums) - 1:
      result.append(nums.copy())
      return

    for index in range(current_index, len(nums)):
      nums[current_index], nums[index] = nums[index], nums[current_index]
      permute_rec(nums, current_index + 1, result)
      nums[current_index], nums[index] = nums[index], nums[current_index]

  permute_rec(nums, 0, result)
  return result
```


```python
import time
# Test the performance
test_input = list(range(10))  # a list from 1 to 9
start_time = time.time()
result = permute(test_input)
end_time = time.time()

# Print the time in milliseconds
print(f"a list of size {len(test_input)} Generate {len(result)} different permutation \nTime taken: {(end_time - start_time) * 1000:.2f} ms")
```

    a list of size 10 Generate 3628800 different permutation 
    Time taken: 6564.17 ms


## 5. Carr√©s latins
---


```python
def generate_combinations(n):
    # Fonction r√©cursive pour g√©n√©rer toutes les combinaisons croissantes
    def recursive_combinations(current_combination, level, start):
        if level == n:  # Si la combinaison atteint la longueur n
            combinations.append(current_combination[:])  # Ajouter une copie de la combinaison
            return
        
        for i in range(start, n+1):  # Toujours aller de start √† n pour √©viter les permutations
            current_combination[level] = i
            recursive_combinations(current_combination, level + 1, i)  # Ne pas revenir en arri√®re

    combinations = []
    current_combination = [0] * n  # Initialiser une combinaison vide de taille n
    recursive_combinations(current_combination, 0, 1)
    return combinations


def find_combinations_with_trace(n, target_trace):
    # G√©n√©rer toutes les combinaisons croissantes de {1, 1, ..., 1} √† {n, n, ..., n}
    all_combinations = generate_combinations(n)
    
    # Filtrer les combinaisons dont la somme des √©l√©ments diagonaux (la trace) correspond √† target_trace
    valid_combinations = []
    for comb in all_combinations:
        trace = sum(comb[i] for i in range(n))  # Calcul de la trace (somme des diagonales)
        if trace == target_trace:
            valid_combinations.append(comb)
    
    return valid_combinations

def initialize_matrix(n):
    i=0
    return [[0 for i in range(n)] for i in range(n)]


def fill_diagonal_with_combination(matrix, combination):
    # Remplir les √©l√©ments diagonaux de la matrice avec une combinaison donn√©e
    n = len(matrix)
    for i in range(n):
        matrix[i][i] = combination[i]  # Affecter chaque √©l√©ment de la combinaison √† la diagonale


def is_safe(matrix, row, col, num, n):
    # V√©rifie si num peut √™tre plac√© dans matrix[row][col] sans redondance dans la ligne et la colonne
    for i in range(n):
        if matrix[row][i] == num or matrix[i][col] == num:
            return False
    return True


def solve_latin_square(matrix, n, row=0, col=0):
    # Si nous avons rempli la derni√®re colonne, passer √† la prochaine ligne
    if col == n:
        col = 0
        row += 1
        if row == n:
            return True  # Matrice remplie sans conflit

    # Si l'√©l√©ment est d√©j√† rempli (appartenant √† la diagonale), passer √† la prochaine case
    if matrix[row][col] != 0:
        return solve_latin_square(matrix, n, row, col + 1)

    # Essayer d'affecter chaque nombre possible de 1 √† n dans la case actuelle
    for num in range(1, n + 1):
        if is_safe(matrix, row, col, num, n):
            matrix[row][col] = num  # Tenter d'affecter num

            # Continuer avec les prochaines cases
            if solve_latin_square(matrix, n, row, col + 1):
                return True

            # Si cela cause un conflit, r√©initialiser la case (backtracking)
            matrix[row][col] = 0

    return False  # Pas de solution possible pour cette configuration


def generate_latin_square(n, target_trace):
    # √âtape 1: G√©n√©rer toutes les combinaisons possibles avec la trace
    valid_combinations = find_combinations_with_trace(n, target_trace)
    
    # √âtape 2: Pour chaque combinaison, remplir la diagonale et compl√©ter la matrice
    for combination in valid_combinations:
        # Initialiser la matrice avec des z√©ros
        matrix = initialize_matrix(n)

        # Remplir la diagonale avec la combinaison actuelle
        fill_diagonal_with_combination(matrix, combination)

        # √âtape 3: Compl√©ter le reste de la matrice en respectant la non-redondance
        if solve_latin_square(matrix, n):
            print("Solution trouv√©e avec la combinaison : ", combination)
            for row in matrix:
                print(row)
            print("\n")
        else:
            print("Impossible de compl√©ter la matrice avec la combinaison : ", combination)
```


```python
n = 3
target_trace = 6
generate_latin_square(n, target_trace)
```

    Solution trouv√©e avec la combinaison :  [1, 2, 3]
    [1, 3, 2]
    [3, 2, 1]
    [2, 1, 3]
    
    
    Solution trouv√©e avec la combinaison :  [2, 2, 2]
    [2, 1, 3]
    [3, 2, 1]
    [1, 3, 2]
    
    


# **Partie 2 "Calcul Scientifique"**


```python
from sklearn.datasets import make_moons, make_circles, make_blobs
import numpy as np
from matplotlib import pyplot as plt
```


```python
datasets = [
make_moons(noise=0.3, random_state=0),
make_circles(noise=0.2, factor=0.5, random_state=1),
make_blobs(n_samples=100, centers=2, n_features=2, center_box=(0, 20), random_state=0)
]
```

## **NumPy: R√©gression Logistique**

### **Code**
---


```python
class LogisticRegression:
    """
    Classe pour impl√©menter la r√©gression logistique avec deux m√©thodes d'optimisation : descente de gradient et m√©thode de Newton.
    """
    def __init__(self, max_iterations=1000, learning_rate=0.01, optimization_method='gradient_descent'):
        self.max_iterations = max_iterations
        self.learning_rate = learning_rate
        self.optimization_method = optimization_method
        self.weights = None
        self.bias = 0
        self.losses = []
        self.accuracies = []

    def sigmoid(self, x):
        """
        Calcule la fonction sigmo√Øde pour une valeur ou un tableau donn√©.
        """
        return 1 / (1 + np.exp(-x))

    def fit(self, X, y):
        """
        Entra√Æne le mod√®le en utilisant les donn√©es d'entr√©e X et les √©tiquettes y.
        La m√©thode d'optimisation choisie est utilis√©e.
        """
        num_samples, num_features = X.shape
        self.weights = np.zeros(num_features)
        self.bias = 0

        if self.optimization_method == 'gradient_descent':
            self.gradient_descent(X, y)
        elif self.optimization_method == 'Newton':
            self.Newton_method(X, y)
        else:
            raise ValueError("M√©thode d'optimisation non reconnue : choisissez 'gradient_descent' ou 'Newton'.")

    def gradient_descent(self, X, y):
        """
        Effectue l'optimisation des poids et du biais en utilisant la m√©thode de descente de gradient.
        """
        for i in range(self.max_iterations):
            # Calcul des pr√©dictions
            linear_model = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_model)

            # Calcul de l'erreur
            error = predictions - y

            # Mise √† jour des poids et du biais
            gradient_w = np.dot(X.T, error) / len(y)
            gradient_b = np.sum(error) / len(y)
            self.weights -= self.learning_rate * gradient_w
            self.bias -= self.learning_rate * gradient_b

            # Calcul de la perte
            loss = -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))
            self.losses.append(loss)

    def Newton_method(self, X, y):
        """
        Effectue l'optimisation des poids et du biais en utilisant la m√©thode de Newton.
        """
        for i in range(self.max_iterations):
            # Calcul des pr√©dictions
            linear_model = np.dot(X, self.weights) + self.bias
            predictions = self.sigmoid(linear_model)

            # Calcul des gradients et de la matrice Hessienne
            gradient_w = np.dot(X.T, predictions - y) / len(y)
            gradient_b = np.sum(predictions - y) / len(y)

            diag = predictions * (1 - predictions)
            hessian = np.dot(X.T, X * diag[:, np.newaxis]) / len(y)

            # Mise √† jour des poids et du biais
            self.weights -= np.linalg.solve(hessian, gradient_w)
            self.bias -= gradient_b

            # Calcul de la perte
            loss = -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))
            self.losses.append(loss)

    def predict(self, X):
        """
        Prend en entr√©e une ou plusieurs donn√©es et retourne les pr√©dictions correspondantes.
        """
        linear_model = np.dot(X, self.weights) + self.bias
        probabilities = self.sigmoid(linear_model)
        return (probabilities >= 0.5).astype(int)

```

### **Documentation : Classe Logistic Regression**
---
**<u>Description</u>** :

La classe LogisticRegression impl√©mente la r√©gression logistique avec deux m√©thodes d‚Äôoptimisation : descente de gradient et m√©thode de Newton. Cette classe est con√ßue pour entra√Æner un mod√®le de classification binaire et effectuer des pr√©dictions.

**<u>Attributs</u>** :

**<span style="color: green;">max_iterations</span>** *(int)*:

Nombre maximal d'it√©rations pour l'entra√Ænement (par d√©faut : 1000).

**<span style="color: green;">learning_rate</span>** *(float)*:

Taux d‚Äôapprentissage utilis√© dans la descente de gradient (par d√©faut : 0.01).

**<span style="color: green;">optimization_method</span>** *(str)*:

M√©thode d'optimisation √† utiliser ('gradient_descent' ou 'Newton' ; par d√©faut : 'gradient_descent').

**<span style="color: green;">weights</span>** *(ndarray)*:

Les poids appris apr√®s l'entra√Ænement.

**<span style="color: green;">bias</span>** *(float)*:

Le biais appris apr√®s l'entra√Ænement.

**<span style="color: green;">losses</span>** *(list)*:

Une liste des valeurs de la perte enregistr√©es au cours des it√©rations.

**<span style="color: green;">accuracies</span>** *(list)*:

Une liste des pr√©cisions calcul√©es pendant les it√©rations (optionnel, non utilis√© actuellement).

**<u>Methods</u>** :

<span style="color: blue;">__init__</span> :

Initialise le mod√®le de r√©gression logistique avec des param√®tres sp√©cifiques.

- **Param√®tres**
  
    - **max_iterations** *(int)*:
 
      Nombre maximal d‚Äôit√©rations pour l‚Äôentra√Ænement.

    - **learning_rate** *(float)*:
 
      Taux d‚Äôapprentissage pour la descente de gradient.
  
    - **optimization_method** *(str)*:
 
      M√©thode d‚Äôoptimisation ('gradient_descent' ou 'Newton').
  

<span style="color: blue;">__sigmoid(self,x)__</span> :


- **Param√®tres**
  
    - **x** *(float or ndarray)*:
 
      La ou les valeurs en entr√©e.
  
- **Retourne**

    - *(float or ndarray)*:

      Le r√©sultat de la fonction sigmo√Øde

  

<span style="color: blue;">__fit(self, X, y)__</span> :

Entra√Æne le mod√®le de r√©gression logistique avec les donn√©es fournies.

- **Param√®tres**
  
    - **X** *(ndarray)*:
 
      Les donn√©es d‚Äôentr√©e, de taille (n_samples, n_features).

    - **y** *(ndarray)*:

      Les √©tiquettes de sortie, de taille (n_samples,).
  
- **Comportement**

    - Utilise la m√©thode d‚Äôoptimisation sp√©cifi√©e.
    - Met √† jour les poids et le biais du mod√®le.
    - Suit la perte √† chaque it√©ration.

<span style="color: blue;">__gradient_descent(self, X, y)__</span> :

Optimise les poids et le biais √† l‚Äôaide de la m√©thode de descente de gradient.

- **Param√®tres**
  
    - **X** *(ndarray)*:
 
      Les donn√©es d‚Äôentr√©e, de taille (n_samples, n_features).

    - **y** *(ndarray)*:

      Les √©tiquettes de sortie, de taille (n_samples,).
  
- **Comportement**

    - Met √† jour les poids et le biais √† l‚Äôaide du gradient de la fonction de perte.
    - Enregistre la perte √† chaque it√©ration.

<span style="color: blue;">__Newton_method(self, X, y)__</span> :

Optimise les poids et le biais √† l‚Äôaide de la m√©thode de Newton.

- **Param√®tres**
  
    - **X** *(ndarray)*:
 
      Les donn√©es d‚Äôentr√©e, de taille (n_samples, n_features).

    - **y** *(ndarray)*:

      Les √©tiquettes de sortie, de taille (n_samples,).
  
- **Comportement**

    - Calcule le gradient et la matrice Hessienne de la fonction de perte.
    - Met √† jour les poids et le biais en utilisant la r√®gle de mise √† jour de Newton.
    - Enregistre la perte √† chaque it√©ration.

<span style="color: blue;">__Predict(self, X)__</span> :

Effectue des pr√©dictions pour les donn√©es fournies en entr√©e.

- **Param√®tres**
  
    - **X** *(ndarray)*:
 
      Les donn√©es d‚Äôentr√©e, de taille (n_samples, n_features).
  
- **Retourne**

    - *(ndarray)*:

        Les pr√©dictions (valeurs binaires : 0 ou 1) pour chaque exemple.

### Test et √©valuation sur les trois  jeux de donn√©es
---


```python
methods = ['gradient_descent', 'Newton']
fig, axs = plt.subplots(len(datasets), 3, figsize=(18, 15))

# Pour chaque ensemble de donn√©es
for i, data in enumerate(datasets):
    X, y = data

    # Pour chaque m√©thode
    for j, method in enumerate(methods):
        # Initialisation du mod√®le de r√©gression logistique
        model = LogisticRegression(max_iterations=1000, optimization_method=method)
        
        # Entra√Ænement du mod√®le sur les donn√©es
        model.fit(X, y)
        
        # Tracer decision boundray
        x1_min, x1_max = X[:, 0].min() - .5, X[:, 0].max() + .5
        x2_min, x2_max = X[:, 1].min() - .5, X[:, 1].max() + .5
        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))
        Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])
        Z = Z.reshape(xx1.shape)
        
        # Visualisation 
        axs[i][j].contourf(xx1, xx2, Z, alpha=0.3, cmap='coolwarm')
        axs[i][j].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='viridis', s=50)  
        axs[i][j].set_xlim(xx1.min(), xx1.max())
        axs[i][j].set_ylim(xx2.min(), xx2.max())
        axs[i][j].set_title(f"{method} - Dataset {i+1}", fontsize=12, fontweight='bold')
        axs[i][j].set_xlabel("X1")
        axs[i][j].set_ylabel("X2")

        # Tracer les courbes de loss
        axs[i][2].plot(model.losses, label=f'{method}', linewidth=2)
        axs[i][2].set_xlabel('It√©ration', fontsize=10)
        axs[i][2].set_ylabel('loss', fontsize=10)
        axs[i][2].set_title(f"Courbes de perte - Dataset {i+1}", fontsize=12, fontweight='bold')
        axs[i][2].grid(True, linestyle='--', alpha=0.6)
        axs[i][2].legend(fontsize=10)

# Ajouter un titre global pour toutes les sous-figures
fig.suptitle('Analyse comparative des m√©thodes d‚Äôoptimisation pour la r√©gression logistique', fontsize=16, fontweight='bold')
fig.tight_layout(rect=[0, 0.03, 1, 0.95], pad=3.0)

plt.show()

```


    
![png](AkramChaabnia_AbdelheqMokhtari_mohamedRaoufRazi_files/AkramChaabnia_AbdelheqMokhtari_mohamedRaoufRazi_31_0.png)
    


**<u>Note</u>**:

- La m√©thode de Newton surpasse la descente de gradient : Elle converge plus rapidement et produit des fronti√®res de d√©cision plus pr√©cises sur tous les datasets.
- La descente de gradient est plus lente : Bien qu'elle soit efficace pour s√©parer les classes, elle n√©cessite beaucoup plus d'it√©rations pour converger, comme le montrent les courbes de perte.
- Complexit√© des datasets : Les deux m√©thodes fonctionnent bien, mais la m√©thode de Newton excelle pour des datasets avec des s√©parations nettes des classes gr√¢ce √† sa rapidit√©.
- Recommandation pratique : Utilisez la m√©thode de Newton pour des datasets de petite ou moyenne taille n√©cessitant une haute pr√©cision. La descente de gradient est mieux adapt√©e aux grands ensembles de donn√©es avec des contraintes de calcul.
